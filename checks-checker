#!/usr/bin/env python3
import argparse
import logging
import re
import sys
import types
from urllib.parse import urlparse
import glob

import requests
import yaml
from bs4 import BeautifulSoup


def getUrlsFromText(text) -> list:
    """Returns URLs found in a text

    Taken from https://www.geeksforgeeks.org/python-check-url-string/

    Parameters:
    text (string): The text to search for URLs

    Returns:
    urls (list): A list of URLs that have been found in the text

    """
    regex = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
    result = re.findall(regex, text)
    urls = [x[0] for x in result]
    return urls

def checkUrl(url: str) -> int:
    """Checks an URL for validity

    Parameters:
    url (string): The text to search for URLs

    Returns:
    errorcount (int): returns 1 if an error was encountered, 0 otherwise

    """
    warnings=False
    parts = urlparse(url)

    # We should/must use the single-html version for SUSE documentation
    # Otherwise links might break if the the document structure changes
    if parts.hostname == "documentation.suse.com":
        if "/single-html/" not in parts.path:
            logging.warning(f"Use the single-html version of the document instead! {url}")
            warnings=True
    fragment=parts.fragment
    try:
        response = requests.get(url)
    except requests.ConnectionError:
        logging.error(f"Connection failed for {url}")
        return 1
    if not response.ok:
        logging.error(f"status {response.status_code} returned for {url}")
        return 1

    # If a URL gets redirected, we might want to 
    # change the documentation link to the redirection target.
    if response.url != url:
        logging.warning(f"got redirected to {response.url} from {url}")
        warnings=True
    htmlDocument=response.text
    if fragment:
        logging.debug(f"fragment is \"{fragment}\"") 
        soup = BeautifulSoup(htmlDocument, 'html.parser')
        htmlElements=soup.find_all(id=fragment)
        if len(htmlElements)==0:
            # Not an id, trying name
            htmlElements=soup.find_all(name=fragment)        
        if len(htmlElements)==0:
            logging.error(f"fragment \"{fragment}\" not found in {url}")
            return 1
        logging.debug(f"found fragment \"{fragment}\"") 
    if not warnings: 
        logging.info(f"no issues with {url}") 
    return 0

def checkCheckUrls(check: dict) -> int:
    """Checks the URLs in a check

    Parameters:
    check (dict): The check to check the URLs of 

    Returns:
    errorcount (int): returns the number of errors encountered, if any

    """
    urlErrors=0
    # Check URLs in remediation text
    if 'remediation' not in check:
        logging.info("Check has no remediation section, skipping check")
        return 0
    urls = getUrlsFromText(check['remediation'])
    logging.debug(f"{len(urls)} URLs found")
    if len(urls)>0:
        for url in urls:
            urlErrors += checkUrl(url)
    return urlErrors

def checkCheck(check: dict) -> int:
    """Checks a check

    Parameters:
    check (dict): The check to check 

    Returns:
    errorcount (int): returns the number of errors encountered, if any

    """
    checkErrors=0
    logging.info(f"checking check {check['id']}")
    checkErrors+=checkCheckUrls(check)
    return checkErrors

def getChecksfromCheckFile(filename) -> list:
    """parses a checks file and returns a list of the contained check(s) as dictionary 

    Parameters:
    filename (string): name of the checks file to parse

    Returns:
    checks (list): checks contained in the parsed file (list of dicts)

    """
    logging.debug(f"processing file {filename}")
    checks=[]
    try:
        with open(filename, 'r') as file:
            docs = yaml.safe_load_all(file)
            if not isinstance(docs, types.GeneratorType):
                logging.warning(f"file \"{filename}\" contains no valid YAML documents, skipping")
                return []
            for check in docs:
                if not check['id']:
                    logging.debug("Check has no id property, skipping")
                    continue
                checks.append(check)
    except FileNotFoundError:
        logging.debug(f"file \"{filename}\" not found, skipping")
    logging.debug(f"found {len(checks)} checks in file \"{filename}\"")
    return checks


def main():

    # Parse Arguments
    parser = argparse.ArgumentParser(description ='Check Wanda checks')
    parser.add_argument(dest='filenames', metavar='filename', nargs='+', help='checks file')
    parser.add_argument('-l', '--log-level', dest='loglevel', default='INFO', action='store', help='set log level')
    args = parser.parse_args()

    # Set up logging
    logger = logging.getLogger()
    logFormatter = logging.Formatter(fmt='%(levelname)s: %(message)s')
    logHandler = logging.StreamHandler()
    logHandler.setFormatter(logFormatter)
    logger.addHandler(logHandler)
    try:
        logger.setLevel(args.loglevel)
    except ValueError:
        logging.critical("Invalid log level: {} (valid levels: DEBUG, INFO, WARNING, ERROR)".format(args.loglevel))
        sys.exit(255)

    allErrors=0
    # Check all files given as parameters
    for pattern in args.filenames:
        for filename in glob.iglob(pattern, recursive=True):
            try:
                checks=getChecksfromCheckFile(filename)
                for check in checks:
                    allErrors += checkCheck(check)
            except IsADirectoryError:
                pass

    logging.shutdown()
    # Return 1 if any erros occured, or 0 if everything was ok
    exit(int(allErrors > 0))


#---------------------------------
if __name__ == '__main__':
    main()
